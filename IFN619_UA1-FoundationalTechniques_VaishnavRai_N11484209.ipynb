{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ef62cbd-2ea0-442e-973e-e50ca1e97fa2",
   "metadata": {
    "cell_name": "header_cell"
   },
   "source": [
    "<div style=\"background:#FFFFEE; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff597d6e-cdac-4d1a-b657-f6d756380ae3",
   "metadata": {
    "cell_name": "title_cell"
   },
   "source": [
    "# IFN619 :: UA1 - Assignment 1 - Foundational techniques (35%)\n",
    "\n",
    "**IMPORTANT:** Refer to the instructions in Canvas module [UA1 - Assignment 1]() *BEFORE* working on this assignment. Ensure that you are familiar with the marking rubric and understand how the grade for this assignment will be awarded according to the criteria.\n",
    "\n",
    "1. Complete and run the code cell below to display your name and student number\n",
    "2. Complete all questions in Part A (by week 6) - you can get assistance from your tutor with this part.\n",
    "3. Ensure that your tutor has verified your understanding of your work in Part A (no later than week 7)\n",
    "4. Complete a full analysis for Part B. Ensure that you use the techniques and libraries/packages that have been used in class.\n",
    "5. Check that you have addressed all of the criteria in the assignment rubric\n",
    "6. Clear all cells and re-run your entire notebook so that cells are sequentially numbered without any errors. **IMPORTANT: Not doing this step risks having your work marked as incomplete!**\n",
    "7. Submit the clean notebook to Canvas **before** 11:55pm the due date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763f6f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Vaishnav Rai (N11484209)</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the following cell with your details and run to produce your personalised header for this assignment\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "first_name = \"Vaishnav\"\n",
    "last_name = \"Rai\"\n",
    "student_number = \"N11484209\"\n",
    "\n",
    "personal_header = f\"<h1>{first_name} {last_name} ({student_number})</h1>\"\n",
    "HTML(personal_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81caa994",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data for both Part A and Part B\n",
    "\n",
    "This assignment uses data from the Queensland Government [Open Data Portal](https://www.data.qld.gov.au). Both parts will use data on [Queensland Wave Monitoring](https://www.qld.gov.au/environment/coasts-waterways/beach/monitoring). Part B will also use data on [Storm tide monitoring](https://www.qld.gov.au/environment/coasts-waterways/beach/storm) You should familiarise yourself with the information on this site to understand the context for the data.\n",
    "\n",
    "For this assignment, you will use the [Coastal Data System - Near real time wave data]((https://www.data.qld.gov.au/dataset/coastal-data-system-near-real-time-wave-data)) and for Part B you will add [Coastal Data System â€“ Near real time storm tide data](https://www.data.qld.gov.au/dataset/coastal-data-system-near-real-time-storm-tide-data). Note that this data will change over the time period of the assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24fbcf4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A\n",
    "\n",
    "#### QUESTION: \n",
    "***What can we learn from the wave height data for South East Queensland, and how might this data be used strategically during a major weather event?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c66666",
   "metadata": {},
   "source": [
    "> *IMPORTANT* For the following task, keep a record of the dates and times where you demonstrated your understanding with your tutor. These should be AFTER you have completed the questions, and BEFORE week 7. Record these below:\n",
    "\n",
    "**Demo for tutor:** |Tuesday| |01/04/2025| |12:00| (Tutorial 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10629964",
   "metadata": {},
   "source": [
    "### [Q1] Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4628a-fd2d-4ae2-b21f-9f6464250b2e",
   "metadata": {},
   "source": [
    "The data for this analysis comes from the [Queensland Government's Coastal Data System]((https://www.data.qld.gov.au/dataset/coastal-data-system-near-real-time-wave-data)), which maintains wave monitoring buoys at various locations along the Queensland coast. I accessed this real-time wave data on **March 28, 2025**, capturing a 7-day window of wave measurements. Understanding wave characteristics (height, period, direction) is crucial because they directly influence coastal processes and potential impacts.\n",
    "When first examining the data, I found it contained 7,663 records with 15 columns including:\n",
    "\n",
    "* Site location and site number\n",
    "* DateTime of measurement\n",
    "* Geographic coordinates (Latitude/Longitude)\n",
    "* Wave measurements (Hsig - significant wave height, Hmax - maximum wave height)\n",
    "* Wave period metrics (Tp - peak period, Tz - zero crossing period)\n",
    "* Direction data (wave direction, current direction)\n",
    "* Water conditions (SST - sea surface temperature, current speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74324bfc-06dc-4a06-b6cf-bb7bd0657eab",
   "metadata": {},
   "source": [
    "**Justification:** Loading necessary libraries: `pandas` for data manipulation and analysis, `plotly.express` and `plotly.graph_objects` for creating interactive visualizations, and `datetime` for handling date and time operations. Importing `make_subplots` allows for creating more complex figures, such as dual-axis plots used later in Part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412fe539-13eb-4783-82f3-d01296b63067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "# Import tools for dual-axis plots\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10086915-ce72-4900-90c9-faae5138d4cd",
   "metadata": {},
   "source": [
    "First access date: 2025-03-28\n",
    "\n",
    "**Justification:** Recording the date when the data was first accessed is crucial for reproducibility, especially when dealing with data sources that are updated over time. This timestamp will help readers understand the specific 7-day window that was analyzed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba516972-b70b-4d42-83b9-d72d807059a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First access date: 2025-03-28\n"
     ]
    }
   ],
   "source": [
    "# [Q1] Read the data\n",
    "# Make a note of the first access date\n",
    "first_access_date = \"2025-03-28\"\n",
    "print(f\"First access date: {first_access_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad908e-d447-4ca0-a899-bc5814fbe9f5",
   "metadata": {},
   "source": [
    "**Justification:** Initially attempting to load data directly from the source URL. This code is commented out in favour of loading a saved file (next cell) to ensure analysis' consistency and reproducibility, as the live data feed changes. \n",
    "\n",
    "---\n",
    "\n",
    "The wave data is then read into a dataframe using `pd.read_csv`. I discovered the first row had information that caused misalignment of columns. Found code for skipping rows `skiprows=1` from [pandas' read_csv documentation]((https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html))\n",
    "This was needed to remove/skip past the metadata header that was in the first row, due to which the columns were not being read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c95c6c27-db22-40af-bbd6-711f6534c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV version of the file directly from the URL into a pandas dataframe\n",
    "# url = \"https://apps.des.qld.gov.au/data-sets/waves/wave-7dayopdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f22d330-aa65-4720-98c2-d558b2a676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave_df = pd.read_csv(url, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8191dda-54dc-4baf-97b7-f7caa6e3b4f8",
   "metadata": {},
   "source": [
    "**Justification:** Using a local CSV ensures reproducibility, because the live data feed changes over time. Loading the wave data from a previously saved local CSV file (`wave_data_2025-03-28.csv`). This ensures that the analysis uses a consistent, static dataset corresponding to the access date noted earlier (March 28, 2025), making the results reproducible regardless of any changes in the live data source since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89373bb8-82a1-4e33-b49e-17f8e1abc294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved wave data from: wave_data_2025-03-28.csv\n"
     ]
    }
   ],
   "source": [
    "# Instead of loading from URL, load from our saved file\n",
    "saved_wave_file = \"wave_data_2025-03-28.csv\"\n",
    "wave_df = pd.read_csv(saved_wave_file)\n",
    "print(f\"Using saved wave data from: {saved_wave_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bf8b1-15bf-437c-b1e3-3629fb47ad1d",
   "metadata": {},
   "source": [
    "**Justification:** Displaying the first few rows of the loaded wave dataframe using `.head()`. This allows for an initial visual inspection of the data's structure, column names (e.g., 'Hsig', 'Hmax', 'Tp'), and sample values, confirming that the data has been loaded correctly into the pandas DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00906d34-3196-431a-9d9a-a4122e5b35f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Site</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>Seconds</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Hsig</th>\n",
       "      <th>Hmax</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Tz</th>\n",
       "      <th>SST</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Current Speed</th>\n",
       "      <th>Current Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-03-21 00:00:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742479200</td>\n",
       "      <td>-26.84682</td>\n",
       "      <td>153.15536</td>\n",
       "      <td>1.226</td>\n",
       "      <td>1.76</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.195</td>\n",
       "      <td>25.65</td>\n",
       "      <td>92.8</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-03-21 00:30:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742481000</td>\n",
       "      <td>-26.84671</td>\n",
       "      <td>153.15536</td>\n",
       "      <td>1.145</td>\n",
       "      <td>2.09</td>\n",
       "      <td>8.33</td>\n",
       "      <td>5.263</td>\n",
       "      <td>25.60</td>\n",
       "      <td>95.6</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-21 01:00:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742482800</td>\n",
       "      <td>-26.84677</td>\n",
       "      <td>153.15536</td>\n",
       "      <td>1.191</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.063</td>\n",
       "      <td>25.55</td>\n",
       "      <td>95.6</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-03-21 01:30:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742484600</td>\n",
       "      <td>-26.84676</td>\n",
       "      <td>153.15534</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.97</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.333</td>\n",
       "      <td>25.55</td>\n",
       "      <td>85.8</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-21 02:00:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742486400</td>\n",
       "      <td>-26.84658</td>\n",
       "      <td>153.15540</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.86</td>\n",
       "      <td>7.69</td>\n",
       "      <td>5.333</td>\n",
       "      <td>25.55</td>\n",
       "      <td>94.2</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             DateTime       Site SiteNumber     Seconds  Latitude  \\\n",
       "0   1  2025-03-21 00:00:00  Caloundra         54  1742479200 -26.84682   \n",
       "1   2  2025-03-21 00:30:00  Caloundra         54  1742481000 -26.84671   \n",
       "2   3  2025-03-21 01:00:00  Caloundra         54  1742482800 -26.84677   \n",
       "3   4  2025-03-21 01:30:00  Caloundra         54  1742484600 -26.84676   \n",
       "4   5  2025-03-21 02:00:00  Caloundra         54  1742486400 -26.84658   \n",
       "\n",
       "   Longitude   Hsig  Hmax    Tp     Tz    SST  Direction  Current Speed  \\\n",
       "0  153.15536  1.226  1.76  9.09  5.195  25.65       92.8          -99.9   \n",
       "1  153.15536  1.145  2.09  8.33  5.263  25.60       95.6          -99.9   \n",
       "2  153.15536  1.191  2.00  9.09  5.063  25.55       95.6          -99.9   \n",
       "3  153.15534  1.103  1.97  9.09  5.333  25.55       85.8          -99.9   \n",
       "4  153.15540  1.144  1.86  7.69  5.333  25.55       94.2          -99.9   \n",
       "\n",
       "   Current Direction  \n",
       "0              -99.9  \n",
       "1              -99.9  \n",
       "2              -99.9  \n",
       "3              -99.9  \n",
       "4              -99.9  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(\"First 5 rows of the data:\")\n",
    "wave_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7af989-d586-461f-8a2d-03ce0888bb42",
   "metadata": {},
   "source": [
    "**Justification:** Cleaning the column names of the wave dataframe by removing any leading or trailing whitespace using `.str.strip()`. This ensures consistency in column references and prevents potential errors that can arise from hidden whitespace when selecting or manipulating columns later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ea0b60b-4a92-4733-a0b2-72efa2557a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names by stripping whitespace\n",
    "wave_df.columns = wave_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f34147-408a-4625-a21f-036cd4a6132d",
   "metadata": {},
   "source": [
    "**Justification:** Checking the dimensions (number of rows and columns) using `.shape` and listing all column names using `.columns.tolist()`. This provides a quick overview of the dataset's size and verifies the variables available for analysis are as expected after loading and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5acacf9-a87d-4060-b2ae-9c558fa4d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (7663, 15)\n",
      "Columns: ['id', 'DateTime', 'Site', 'SiteNumber', 'Seconds', 'Latitude', 'Longitude', 'Hsig', 'Hmax', 'Tp', 'Tz', 'SST', 'Direction', 'Current Speed', 'Current Direction']\n"
     ]
    }
   ],
   "source": [
    "# Check the shape and columns\n",
    "print(f\"Data shape: {wave_df.shape}\")\n",
    "print(f\"Columns: {wave_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def92bb4-0002-4d67-ac82-3c4048d4648b",
   "metadata": {},
   "source": [
    "**Justification:** Examining the data types (`dtypes`) of each column. This is essential to ensure that numerical columns (like 'Hsig', 'Tp') are recognized as numbers (float64/int64) and categorical columns (like 'Site') are appropriate. This step identifies the 'DateTime' column as being of type 'object' (string), indicating it needs conversion to a proper datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50d8ac03-c8be-48e4-923f-5c2826464476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                     int64\n",
       "DateTime              object\n",
       "Site                  object\n",
       "SiteNumber            object\n",
       "Seconds                int64\n",
       "Latitude             float64\n",
       "Longitude            float64\n",
       "Hsig                 float64\n",
       "Hmax                 float64\n",
       "Tp                   float64\n",
       "Tz                   float64\n",
       "SST                  float64\n",
       "Direction            float64\n",
       "Current Speed        float64\n",
       "Current Direction    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types to understand the structure\n",
    "print(\"\\nData types:\")\n",
    "wave_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5058c7b-9be0-4ae3-ad9c-9b871429309b",
   "metadata": {},
   "source": [
    "**Justification:** Converting the 'DateTime' column from its current 'object' type to a pandas datetime format using `pd.to_datetime`. Using `format='mixed'` allows pandas to infer the format, providing robustness if there are minor inconsistencies in the source data's date strings. Displaying `.head()` of the converted column confirms the successful transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**Regarding Index Column:** In this analysis, I did not use 'DateTime' or any other column as my DataFrame index because they are not guaranteed to be unique and multiple sites record data at the same time. Certain pandas operations (like some forms of resampling or merging) behave unpredictably when indices are non-unique.\n",
    "\n",
    "Instead, I kept 'DateTime' as a normal column and used the original 'id' to ensure each row remains uniquely identifiable. This approach simplifies comparisons across different sites and timestamps without forcing a strictly unique time index. If I need to do time-based queries or visualizations, I can still filter or group by the 'DateTime' column directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3e3239a-7d63-4207-8d1f-6857615abc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datetime conversion successful. Sample dates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   2025-03-21 00:00:00\n",
       "1   2025-03-21 00:30:00\n",
       "2   2025-03-21 01:00:00\n",
       "3   2025-03-21 01:30:00\n",
       "4   2025-03-21 02:00:00\n",
       "Name: DateTime, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DateTime to proper datetime format\n",
    "wave_df['DateTime'] = pd.to_datetime(wave_df['DateTime'], format='mixed')\n",
    "print(\"\\nDatetime conversion successful. Sample dates:\")\n",
    "wave_df['DateTime'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628574f-f275-48ed-95fe-8548f3c9f3df",
   "metadata": {},
   "source": [
    "**Justification:** This commented out code is to save the current `wave_df` into a CSV file so that we can save it locally, and call the CSV file in the cells ahead. This ensures reproducibilty and consistency in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4db7fd45-cb5f-4420-8d9e-94168a347f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q2] Save the data\n",
    "# Create a filename with today's date\n",
    "#filename = f\"wave_data_{first_access_date}.csv\"\n",
    "#wave_df_with_index.to_csv(filename)\n",
    "#print(f\"\\nData saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603f5c9",
   "metadata": {},
   "source": [
    "### Read the data from a file\n",
    "\n",
    "**Justification:** Reading the previously saved wave data CSV back into a new dataframe (`saved_df`). This serves as a check to confirm that the file saving process (like in the previous commented-out cell) worked correctly and the data can be retrieved. For the main analysis continuity, operations will proceed using the primary dataframe (`wave_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "402252c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully read back from file.\n"
     ]
    }
   ],
   "source": [
    "# Read the data back from file\n",
    "saved_df = pd.read_csv(\"wave_data_2025-03-28.csv\")\n",
    "print(\"Data successfully read back from file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b6698-02af-4d38-9349-7a2297f210ca",
   "metadata": {},
   "source": [
    "### [Q3] Analyse the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbc989-0b49-46e7-bd67-61f238a3782b",
   "metadata": {},
   "source": [
    "\n",
    "**Justification:** Printing all unique site names present in the dataset provides a complete list of available monitoring locations, which helps in selecting the relevant sites for the South East Queensland focus required by the analysis question. We need to isolate which monitoring stations lie in the South East Queensland. This helps us focus on the wave data most relevant to emergency management in that area.\n",
    "\n",
    "This initial examination shows us that the dataset contains records from 20 unique monitoring stations along the Queensland coast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e80f3243-a28f-4e76-a3d9-68a6051fe9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sites:\n",
      "- Albatross Bay\n",
      "- Bilinga\n",
      "- Brisbane Mk4\n",
      "- Bundaberg\n",
      "- Cairns Mk4\n",
      "- Caloundra\n",
      "- Emu Park\n",
      "- Gladstone\n",
      "- Gold Coast Mk4\n",
      "- Hay Point TriAxys\n",
      "- Mackay Mk4\n",
      "- Mooloolaba\n",
      "- North Moreton Bay\n",
      "- Palm Beach Mk4\n",
      "- Poruma West\n",
      "- Skardon River Outer\n",
      "- Townsville\n",
      "- Tweed Heads Mk4\n",
      "- Tweed Offshore\n",
      "- Wide Bay\n"
     ]
    }
   ],
   "source": [
    "# [Q3] Analyze the data\n",
    "# First, let's check the unique sites to identify South East coast sites\n",
    "print(\"Available sites:\")\n",
    "for site in sorted(wave_df['Site'].unique()):\n",
    "    print(f\"- {site}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41226214-9da4-4cfc-bde0-a9e8c90874c4",
   "metadata": {},
   "source": [
    "**Justification:** Defining a list (`se_qld_sites`) containing the specific site names relevant to the South East Queensland coast (Gold Coast to Sunshine Coast) as identified for this analysis. This list will be used to filter the main dataframe. I filtered the data to include only the relevant coastal sites. For this analysis, I'll focus specifically on the 9 stations in South East Queensland to address our regional question. I used the [Wave Monitoring Website](https://www.qld.gov.au/environment/coasts-waterways/beach/monitoring/waves-sites) to find the sites on the South East Queensland coast. The website clearly names the wave stations that are in Sunshine Coast, Brisbane, and Gold Coast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fd0d930-0e40-44ff-91a9-77a794005017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define South East Queensland sites (Gold Coast to Sunshine Coast)\n",
    "se_qld_sites = [\n",
    "    'Brisbane Mk4', 'Caloundra', 'Gold Coast Mk4', 'Mooloolaba', 'North Moreton Bay' , \n",
    "    'Tweed Heads Mk4','Tweed Offshore', 'Palm Beach Mk4', 'Bilinga'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30225f2-c666-4f34-ae46-54cadbfc4e08",
   "metadata": {},
   "source": [
    "**Justification:** Filtering the wave dataframe (`wave_df`) based on the predefined list of relevant sites (`se_qld_sites`). This creates a new dataframe (`se_qld_df`) containing only the wave measurements for the South East Queensland region. Displaying the shape of the filtered data and the count of records per site (`.value_counts()`) confirms the filtering was successful and provides an overview of the data available for each targeted location. We can see that all of our sites have a similar number of records, with discrepancies of 1-5 data points missing at few sites like North Moreton Bay. Palm Beach Mk4 and Bilinga amongst others have 379 records. This near-complete coverage is beneficial for emergency managers, as data gaps might hide peak wave events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfce3ba3-7115-417d-95e7-4d6802c04f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered data shape: (3400, 15)\n",
      "Number of records by site:\n",
      "Site\n",
      "Tweed Heads Mk4      379\n",
      "Palm Beach Mk4       379\n",
      "Gold Coast Mk4       379\n",
      "Bilinga              379\n",
      "Mooloolaba           378\n",
      "Brisbane Mk4         378\n",
      "Tweed Offshore       378\n",
      "Caloundra            376\n",
      "North Moreton Bay    374\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter data for matched SE QLD sites\n",
    "se_qld_df = wave_df[wave_df['Site'].isin(se_qld_sites)]\n",
    "\n",
    "# Show the filtered data size\n",
    "print(f\"\\nFiltered data shape: {se_qld_df.shape}\")\n",
    "print(f\"Number of records by site:\")\n",
    "print(se_qld_df['Site'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bb104-7233-4f8e-b709-7225eaf8a5ab",
   "metadata": {},
   "source": [
    "**Justification:** Grouping the filtered South East Queensland wave data (`se_qld_df`) by 'Site' and calculating key aggregate statistics for each site over the entire 7-day period. The aggregates include mean, maximum, and minimum Significant Wave Height ('Hsig'), mean and maximum Maximum Wave Height ('Hmax'), mean Peak Period ('Tp'), and mean Direction. This process condenses thousands of time-series records into a manageable summary table, allowing for direct comparison of typical conditions, peak conditions, wave periods, and directions across the different SE QLD sites. `.reset_index()` converts the grouped output back into a standard dataframe. Flattening the hierarchical column index (created by the multi-level aggregation) makes column names simpler and easier to use. Sorting the resulting dataframe by mean Significant Wave Height (`Hsig mean`) arranges the sites from highest to lowest average wave exposure that helps us with interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "***Note**: GenAI (Claude) was used to assist in generating the code for flattening the hierarchical column index after aggregation and for sorting by the mean Hsig.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc38ac5-f38c-448c-b86f-7dc6ee255bea",
   "metadata": {},
   "source": [
    "**Choice and Value of Aggregations:**\n",
    "\n",
    "To effectively summarize the wave climate during the observation period (March 21-28, 2025) and extract meaningful insights, the following aggregations were calculated by site.\n",
    "\n",
    "---\n",
    "\n",
    "*   **Significant Wave Height (Mean, Max, Min):** Provides a measure of the average wave energy (Mean), the peak intensity observed (Max), and baseline conditions (Min). Crucial for understanding both typical exposure and extreme event characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "*   **Maximum Wave Height (Mean, Max):** Offers insight into the typical size of the largest waves (Mean Hmax) and the single highest wave recorded (Max Hmax), important for safety thresholds and structural considerations.\n",
    "\n",
    "---\n",
    "\n",
    "*   **Peak Period (Mean):** Indicates the dominant wave period, helping to infer the nature of the waves (e.g., shorter period wind waves vs. longer period swell). Longer period waves generally carry more energy.\n",
    "\n",
    "---\n",
    "\n",
    "*   **Wave Direction (Mean):** Shows the predominant direction waves approached from, critical for understanding coastal alignment impacts and potential erosion patterns.\n",
    "\n",
    "---\n",
    "\n",
    "**Strategic Value:** These aggregations offer a multi-dimensional perspective on the wave environment. They enable comparison of sites based on typical and extreme wave exposure, provide context for assessing coastal risk, help inform resource allocation during storm events by identifying higher-impact zones, and contribute to public safety messaging by characterizing wave conditions.  By identifying the site with the highest average Hsig and the largest max Hmax, we can see which coastal areas are likely most impacted during a severe storm. We will discuss more during the insights section\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67e0ac9a-60c0-4c9a-8993-1e6793fa1e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wave statistics by site in South East Queensland:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>Hsig mean</th>\n",
       "      <th>Hsig max</th>\n",
       "      <th>Hsig min</th>\n",
       "      <th>Hmax mean</th>\n",
       "      <th>Hmax max</th>\n",
       "      <th>Tp mean</th>\n",
       "      <th>Direction mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mooloolaba</td>\n",
       "      <td>1.413862</td>\n",
       "      <td>2.462</td>\n",
       "      <td>0.740</td>\n",
       "      <td>2.445450</td>\n",
       "      <td>4.88</td>\n",
       "      <td>7.140370</td>\n",
       "      <td>89.240212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brisbane Mk4</td>\n",
       "      <td>1.336058</td>\n",
       "      <td>2.230</td>\n",
       "      <td>0.760</td>\n",
       "      <td>2.267011</td>\n",
       "      <td>4.19</td>\n",
       "      <td>7.063201</td>\n",
       "      <td>80.632302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tweed Offshore</td>\n",
       "      <td>1.246243</td>\n",
       "      <td>1.980</td>\n",
       "      <td>0.720</td>\n",
       "      <td>2.107831</td>\n",
       "      <td>3.67</td>\n",
       "      <td>7.810159</td>\n",
       "      <td>107.193254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>North Moreton Bay</td>\n",
       "      <td>1.144957</td>\n",
       "      <td>2.035</td>\n",
       "      <td>0.623</td>\n",
       "      <td>1.972032</td>\n",
       "      <td>3.70</td>\n",
       "      <td>7.075829</td>\n",
       "      <td>75.054813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tweed Heads Mk4</td>\n",
       "      <td>1.117757</td>\n",
       "      <td>1.790</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1.886781</td>\n",
       "      <td>3.24</td>\n",
       "      <td>7.755673</td>\n",
       "      <td>94.068285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caloundra</td>\n",
       "      <td>1.108457</td>\n",
       "      <td>1.913</td>\n",
       "      <td>0.591</td>\n",
       "      <td>1.889229</td>\n",
       "      <td>3.34</td>\n",
       "      <td>7.096410</td>\n",
       "      <td>83.257979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gold Coast Mk4</td>\n",
       "      <td>1.104433</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.590</td>\n",
       "      <td>1.871979</td>\n",
       "      <td>3.29</td>\n",
       "      <td>8.008654</td>\n",
       "      <td>89.093140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Palm Beach Mk4</td>\n",
       "      <td>1.030739</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.758391</td>\n",
       "      <td>3.16</td>\n",
       "      <td>6.806464</td>\n",
       "      <td>79.144063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bilinga</td>\n",
       "      <td>0.927441</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0.480</td>\n",
       "      <td>1.567731</td>\n",
       "      <td>3.12</td>\n",
       "      <td>6.692902</td>\n",
       "      <td>70.497942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Site  Hsig mean  Hsig max  Hsig min  Hmax mean  Hmax max  \\\n",
       "4         Mooloolaba   1.413862     2.462     0.740   2.445450      4.88   \n",
       "1       Brisbane Mk4   1.336058     2.230     0.760   2.267011      4.19   \n",
       "8     Tweed Offshore   1.246243     1.980     0.720   2.107831      3.67   \n",
       "5  North Moreton Bay   1.144957     2.035     0.623   1.972032      3.70   \n",
       "7    Tweed Heads Mk4   1.117757     1.790     0.630   1.886781      3.24   \n",
       "2          Caloundra   1.108457     1.913     0.591   1.889229      3.34   \n",
       "3     Gold Coast Mk4   1.104433     1.750     0.590   1.871979      3.29   \n",
       "6     Palm Beach Mk4   1.030739     1.740     0.570   1.758391      3.16   \n",
       "0            Bilinga   0.927441     1.600     0.480   1.567731      3.12   \n",
       "\n",
       "    Tp mean  Direction mean  \n",
       "4  7.140370       89.240212  \n",
       "1  7.063201       80.632302  \n",
       "8  7.810159      107.193254  \n",
       "5  7.075829       75.054813  \n",
       "7  7.755673       94.068285  \n",
       "2  7.096410       83.257979  \n",
       "3  8.008654       89.093140  \n",
       "6  6.806464       79.144063  \n",
       "0  6.692902       70.497942  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by site and calculate mean wave heights and other statistics\n",
    "grouped_df = se_qld_df.groupby('Site').agg({\n",
    "    'Hsig': ['mean', 'max', 'min'],  # Significant wave height\n",
    "    'Hmax': ['mean', 'max'],        # Maximum wave height\n",
    "    'Tp': 'mean',                   # Peak period\n",
    "    'Direction': 'mean'             # Wave direction\n",
    "}).reset_index()\n",
    "# Flatten the column hierarchy\n",
    "grouped_df.columns = [' '.join(col).strip() if col[1] else col[0] for col in grouped_df.columns.values]\n",
    "\n",
    "# Sort by mean significant wave height (descending)\n",
    "grouped_df = grouped_df.sort_values('Hsig mean', ascending=False)\n",
    "\n",
    "print(\"\\nWave statistics by site in South East Queensland:\")\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabe6de-7f46-404f-bc8f-4935d78adf40",
   "metadata": {},
   "source": [
    "This reveals both typical conditions and extremes for each location over the observed week (March 21-28, 2025). Key patterns relevant to coastal management include:\n",
    "\n",
    "1.  **Exposure:** A clear gradient exists, with Mooloolaba (mean Significant Wave Height 1.41m) and Brisbane Mk4 (1.34m) showing the highest average wave energy exposure in this dataset. Bilinga (0.93m) consistently experienced the most sheltered conditions.\n",
    "\n",
    "---\n",
    "\n",
    "2.  **Peak Event Intensity:** Mooloolaba recorded the highest significant wave height (max 2.46m) which likely occurred around March 26 afternoon (we will confirm in a time-series plot down below). This might coincide with a local storm event. It also received highest maximum wave height (max 4.88m).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "3.  **Wave Period:** Mean peak periods mostly range from 6.69s to 8.01s, suggesting a mix of conditions potentially including local wind waves and some swell.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "4.  **Predominant Direction:** Mean wave directions generally fall within the East to North-East sector (approx. 70Â° to 94Â° for most sites, Tweed Offshore 107Â°).\n",
    "\n",
    "---\n",
    "5.  **Synchronized Patterns (See Time Series Plot):** The timing of wave height changes appears broadly synchronized across sites, suggesting the influence of a regional weather system affecting the coastline.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256cee8-7a92-44f7-bb48-0a2014623f55",
   "metadata": {},
   "source": [
    "**Justification:** This commented-out code block demonstrates how the *aggregated* wave statistics dataframe (`grouped_df`) could be saved to a CSV file. This preserves the summarized results derived from the analysis for reporting or later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cdd9075-1ac3-4d0a-ac9b-a233b504f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the grouped data to a CSV with the date\n",
    "# Not Necessary but for personal use\n",
    "# grouped_filename = f\"wave_data_grouped_{first_access_date}.csv\"\n",
    "# grouped_df.to_csv(grouped_filename, index=False)\n",
    "# print(f\"Grouped data saved to {grouped_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521b40b-98d0-4a9d-a860-434ada8b8c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda31e8-3a3b-4e83-bc0b-bdfb75d9497b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a92ce-f2af-48ad-9f1c-085f7b1e6c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2885e1b-42ae-49e4-815e-2a291761a87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada32188-0dfe-4437-85b5-083526bcec1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62232ff-7640-4084-ac77-700e51ef2553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec3d9f-d156-4e3d-988b-aeff3ee482a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "creation_period": "",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nb_name": "template",
  "qut": {
   "creation_period": "2023_sem1",
   "nb_name": "template-assignment1",
   "unit_code": "IFN619"
  },
  "unit_code": "",
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
