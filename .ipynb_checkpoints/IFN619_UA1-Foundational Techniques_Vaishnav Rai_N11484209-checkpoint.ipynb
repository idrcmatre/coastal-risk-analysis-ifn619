{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ef62cbd-2ea0-442e-973e-e50ca1e97fa2",
   "metadata": {
    "cell_name": "header_cell"
   },
   "source": [
    "<div style=\"background:#FFFFEE; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff597d6e-cdac-4d1a-b657-f6d756380ae3",
   "metadata": {
    "cell_name": "title_cell"
   },
   "source": [
    "# IFN619 :: UA1 - Assignment 1 - Foundational techniques (35%)\n",
    "\n",
    "**IMPORTANT:** Refer to the instructions in Canvas module [UA1 - Assignment 1]() *BEFORE* working on this assignment. Ensure that you are familiar with the marking rubric and understand how the grade for this assignment will be awarded according to the criteria.\n",
    "\n",
    "1. Complete and run the code cell below to display your name and student number\n",
    "2. Complete all questions in Part A (by week 6) - you can get assistance from your tutor with this part.\n",
    "3. Ensure that your tutor has verified your understanding of your work in Part A (no later than week 7)\n",
    "4. Complete a full analysis for Part B. Ensure that you use the techniques and libraries/packages that have been used in class.\n",
    "5. Check that you have addressed all of the criteria in the assignment rubric\n",
    "6. Clear all cells and re-run your entire notebook so that cells are sequentially numbered without any errors. **IMPORTANT: Not doing this step risks having your work marked as incomplete!**\n",
    "7. Submit the clean notebook to Canvas **before** 11:55pm the due date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763f6f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Vaishnav Rai (N11484209)</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the following cell with your details and run to produce your personalised header for this assignment\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "first_name = \"Vaishnav\"\n",
    "last_name = \"Rai\"\n",
    "student_number = \"N11484209\"\n",
    "\n",
    "personal_header = f\"<h1>{first_name} {last_name} ({student_number})</h1>\"\n",
    "HTML(personal_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81caa994",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data for both Part A and Part B\n",
    "\n",
    "This assignment uses data from the Queensland Government [Open Data Portal](https://www.data.qld.gov.au). Both parts will use data on [Queensland Wave Monitoring](https://www.qld.gov.au/environment/coasts-waterways/beach/monitoring). Part B will also use data on [Storm tide monitoring](https://www.qld.gov.au/environment/coasts-waterways/beach/storm) You should familiarise yourself with the information on this site to understand the context for the data.\n",
    "\n",
    "For this assignment, you will use the [Coastal Data System - Near real time wave data]((https://www.data.qld.gov.au/dataset/coastal-data-system-near-real-time-wave-data)) and for Part B you will add [Coastal Data System â€“ Near real time storm tide data](https://www.data.qld.gov.au/dataset/coastal-data-system-near-real-time-storm-tide-data). Note that this data will change over the time period of the assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24fbcf4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A\n",
    "\n",
    "#### QUESTION: \n",
    "***What can we learn from the wave height data for South East Queensland, and how might this data be used strategically during a major weather event?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c66666",
   "metadata": {},
   "source": [
    "> *IMPORTANT* For the following task, keep a record of the dates and times where you demonstrated your understanding with your tutor. These should be AFTER you have completed the questions, and BEFORE week 7. Record these below:\n",
    "\n",
    "**Demo for tutor:** |Tuesday| |01/04/2025| |12:00| (Tutorial 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10629964",
   "metadata": {},
   "source": [
    "### [Q1] Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4628a-fd2d-4ae2-b21f-9f6464250b2e",
   "metadata": {},
   "source": [
    "The data for this analysis comes from the [Queensland Government's Coastal Data System]((https://www.data.qld.gov.au/dataset/coastal-data-system-near-real-time-wave-data)), which maintains wave monitoring buoys at various locations along the Queensland coast. I accessed this real-time wave data on **March 28, 2025**, capturing a 7-day window of wave measurements. Understanding wave characteristics (height, period, direction) is crucial because they directly influence coastal processes and potential impacts.\n",
    "When first examining the data, I found it contained 7,663 records with 15 columns including:\n",
    "\n",
    "* Site location and site number\n",
    "* DateTime of measurement\n",
    "* Geographic coordinates (Latitude/Longitude)\n",
    "* Wave measurements (Hsig - significant wave height, Hmax - maximum wave height)\n",
    "* Wave period metrics (Tp - peak period, Tz - zero crossing period)\n",
    "* Direction data (wave direction, current direction)\n",
    "* Water conditions (SST - sea surface temperature, current speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74324bfc-06dc-4a06-b6cf-bb7bd0657eab",
   "metadata": {},
   "source": [
    "**Justification:** Loading necessary libraries: `pandas` for data manipulation and analysis, `plotly.express` and `plotly.graph_objects` for creating interactive visualizations, and `datetime` for handling date and time operations. Importing `make_subplots` allows for creating more complex figures, such as dual-axis plots used later in Part B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412fe539-13eb-4783-82f3-d01296b63067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "# Import tools for dual-axis plots\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10086915-ce72-4900-90c9-faae5138d4cd",
   "metadata": {},
   "source": [
    "First access date: 2025-03-28\n",
    "\n",
    "**Justification:** Recording the date when the data was first accessed is crucial for reproducibility, especially when dealing with data sources that are updated over time. This timestamp will help readers understand the specific 7-day window that was analyzed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba516972-b70b-4d42-83b9-d72d807059a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First access date: 2025-03-28\n"
     ]
    }
   ],
   "source": [
    "# [Q1] Read the data\n",
    "# Make a note of the first access date\n",
    "first_access_date = \"2025-03-28\"\n",
    "print(f\"First access date: {first_access_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad908e-d447-4ca0-a899-bc5814fbe9f5",
   "metadata": {},
   "source": [
    "**Justification:** Initially attempting to load data directly from the source URL. This code is commented out in favour of loading a saved file (next cell) to ensure analysis' consistency and reproducibility, as the live data feed changes. \n",
    "\n",
    "---\n",
    "\n",
    "The wave data is then read into a dataframe using `pd.read_csv`. I discovered the first row had information that caused misalignment of columns. Found code for skipping rows `skiprows=1` from [pandas' read_csv documentation]((https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html))\n",
    "This was needed to remove/skip past the metadata header that was in the first row, due to which the columns were not being read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95c6c27-db22-40af-bbd6-711f6534c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV version of the file directly from the URL into a pandas dataframe\n",
    "# url = \"https://apps.des.qld.gov.au/data-sets/waves/wave-7dayopdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f22d330-aa65-4720-98c2-d558b2a676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave_df = pd.read_csv(url, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8191dda-54dc-4baf-97b7-f7caa6e3b4f8",
   "metadata": {},
   "source": [
    "**Justification:** Using a local CSV ensures reproducibility, because the live data feed changes over time. Loading the wave data from a previously saved local CSV file (`wave_data_2025-03-28.csv`). This ensures that the analysis uses a consistent, static dataset corresponding to the access date noted earlier (March 28, 2025), making the results reproducible regardless of any changes in the live data source since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89373bb8-82a1-4e33-b49e-17f8e1abc294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved wave data from: wave_data_2025-03-28.csv\n"
     ]
    }
   ],
   "source": [
    "# Instead of loading from URL, load from our saved file\n",
    "saved_wave_file = \"wave_data_2025-03-28.csv\"\n",
    "wave_df = pd.read_csv(saved_wave_file)\n",
    "print(f\"Using saved wave data from: {saved_wave_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bf8b1-15bf-437c-b1e3-3629fb47ad1d",
   "metadata": {},
   "source": [
    "**Justification:** Displaying the first few rows of the loaded wave dataframe using `.head()`. This allows for an initial visual inspection of the data's structure, column names (e.g., 'Hsig', 'Hmax', 'Tp'), and sample values, confirming that the data has been loaded correctly into the pandas DataFrame structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00906d34-3196-431a-9d9a-a4122e5b35f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Site</th>\n",
       "      <th>SiteNumber</th>\n",
       "      <th>Seconds</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Hsig</th>\n",
       "      <th>Hmax</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Tz</th>\n",
       "      <th>SST</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Current Speed</th>\n",
       "      <th>Current Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-03-21 00:00:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742479200</td>\n",
       "      <td>-26.84682</td>\n",
       "      <td>153.15536</td>\n",
       "      <td>1.226</td>\n",
       "      <td>1.76</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.195</td>\n",
       "      <td>25.65</td>\n",
       "      <td>92.8</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-03-21 00:30:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742481000</td>\n",
       "      <td>-26.84671</td>\n",
       "      <td>153.15536</td>\n",
       "      <td>1.145</td>\n",
       "      <td>2.09</td>\n",
       "      <td>8.33</td>\n",
       "      <td>5.263</td>\n",
       "      <td>25.60</td>\n",
       "      <td>95.6</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-21 01:00:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742482800</td>\n",
       "      <td>-26.84677</td>\n",
       "      <td>153.15536</td>\n",
       "      <td>1.191</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.063</td>\n",
       "      <td>25.55</td>\n",
       "      <td>95.6</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-03-21 01:30:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742484600</td>\n",
       "      <td>-26.84676</td>\n",
       "      <td>153.15534</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.97</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.333</td>\n",
       "      <td>25.55</td>\n",
       "      <td>85.8</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-21 02:00:00</td>\n",
       "      <td>Caloundra</td>\n",
       "      <td>54</td>\n",
       "      <td>1742486400</td>\n",
       "      <td>-26.84658</td>\n",
       "      <td>153.15540</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.86</td>\n",
       "      <td>7.69</td>\n",
       "      <td>5.333</td>\n",
       "      <td>25.55</td>\n",
       "      <td>94.2</td>\n",
       "      <td>-99.9</td>\n",
       "      <td>-99.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             DateTime       Site SiteNumber     Seconds  Latitude  \\\n",
       "0   1  2025-03-21 00:00:00  Caloundra         54  1742479200 -26.84682   \n",
       "1   2  2025-03-21 00:30:00  Caloundra         54  1742481000 -26.84671   \n",
       "2   3  2025-03-21 01:00:00  Caloundra         54  1742482800 -26.84677   \n",
       "3   4  2025-03-21 01:30:00  Caloundra         54  1742484600 -26.84676   \n",
       "4   5  2025-03-21 02:00:00  Caloundra         54  1742486400 -26.84658   \n",
       "\n",
       "   Longitude   Hsig  Hmax    Tp     Tz    SST  Direction  Current Speed  \\\n",
       "0  153.15536  1.226  1.76  9.09  5.195  25.65       92.8          -99.9   \n",
       "1  153.15536  1.145  2.09  8.33  5.263  25.60       95.6          -99.9   \n",
       "2  153.15536  1.191  2.00  9.09  5.063  25.55       95.6          -99.9   \n",
       "3  153.15534  1.103  1.97  9.09  5.333  25.55       85.8          -99.9   \n",
       "4  153.15540  1.144  1.86  7.69  5.333  25.55       94.2          -99.9   \n",
       "\n",
       "   Current Direction  \n",
       "0              -99.9  \n",
       "1              -99.9  \n",
       "2              -99.9  \n",
       "3              -99.9  \n",
       "4              -99.9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "print(\"First 5 rows of the data:\")\n",
    "wave_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7af989-d586-461f-8a2d-03ce0888bb42",
   "metadata": {},
   "source": [
    "**Justification:** Cleaning the column names of the wave dataframe by removing any leading or trailing whitespace using `.str.strip()`. This ensures consistency in column references and prevents potential errors that can arise from hidden whitespace when selecting or manipulating columns later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea0b60b-4a92-4733-a0b2-72efa2557a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names by stripping whitespace\n",
    "wave_df.columns = wave_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f34147-408a-4625-a21f-036cd4a6132d",
   "metadata": {},
   "source": [
    "**Justification:** Checking the dimensions (number of rows and columns) using `.shape` and listing all column names using `.columns.tolist()`. This provides a quick overview of the dataset's size and verifies the variables available for analysis are as expected after loading and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5acacf9-a87d-4060-b2ae-9c558fa4d521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (7663, 15)\n",
      "Columns: ['id', 'DateTime', 'Site', 'SiteNumber', 'Seconds', 'Latitude', 'Longitude', 'Hsig', 'Hmax', 'Tp', 'Tz', 'SST', 'Direction', 'Current Speed', 'Current Direction']\n"
     ]
    }
   ],
   "source": [
    "# Check the shape and columns\n",
    "print(f\"Data shape: {wave_df.shape}\")\n",
    "print(f\"Columns: {wave_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def92bb4-0002-4d67-ac82-3c4048d4648b",
   "metadata": {},
   "source": [
    "**Justification:** Examining the data types (`dtypes`) of each column. This is essential to ensure that numerical columns (like 'Hsig', 'Tp') are recognized as numbers (float64/int64) and categorical columns (like 'Site') are appropriate. This step identifies the 'DateTime' column as being of type 'object' (string), indicating it needs conversion to a proper datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d8ac03-c8be-48e4-923f-5c2826464476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                     int64\n",
       "DateTime              object\n",
       "Site                  object\n",
       "SiteNumber            object\n",
       "Seconds                int64\n",
       "Latitude             float64\n",
       "Longitude            float64\n",
       "Hsig                 float64\n",
       "Hmax                 float64\n",
       "Tp                   float64\n",
       "Tz                   float64\n",
       "SST                  float64\n",
       "Direction            float64\n",
       "Current Speed        float64\n",
       "Current Direction    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types to understand the structure\n",
    "print(\"\\nData types:\")\n",
    "wave_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5058c7b-9be0-4ae3-ad9c-9b871429309b",
   "metadata": {},
   "source": [
    "**Justification:** Converting the 'DateTime' column from its current 'object' type to a pandas datetime format using `pd.to_datetime`. Using `format='mixed'` allows pandas to infer the format, providing robustness if there are minor inconsistencies in the source data's date strings. Displaying `.head()` of the converted column confirms the successful transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**Regarding Index Column:** In this analysis, I did not use 'DateTime' or any other column as my DataFrame index because they are not guaranteed to be unique and multiple sites record data at the same time. Certain pandas operations (like some forms of resampling or merging) behave unpredictably when indices are non-unique.\n",
    "\n",
    "Instead, I kept 'DateTime' as a normal column and used the original 'id' to ensure each row remains uniquely identifiable. This approach simplifies comparisons across different sites and timestamps without forcing a strictly unique time index. If I need to do time-based queries or visualizations, I can still filter or group by the 'DateTime' column directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e3239a-7d63-4207-8d1f-6857615abc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datetime conversion successful. Sample dates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   2025-03-21 00:00:00\n",
       "1   2025-03-21 00:30:00\n",
       "2   2025-03-21 01:00:00\n",
       "3   2025-03-21 01:30:00\n",
       "4   2025-03-21 02:00:00\n",
       "Name: DateTime, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DateTime to proper datetime format\n",
    "wave_df['DateTime'] = pd.to_datetime(wave_df['DateTime'], format='mixed')\n",
    "print(\"\\nDatetime conversion successful. Sample dates:\")\n",
    "wave_df['DateTime'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628574f-f275-48ed-95fe-8548f3c9f3df",
   "metadata": {},
   "source": [
    "**Justification:** This commented out code is to save the current `wave_df` into a CSV file so that we can save it locally, and call the CSV file in the cells ahead. This ensures reproducibilty and consistency in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db7fd45-cb5f-4420-8d9e-94168a347f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Q2] Save the data\n",
    "# Create a filename with today's date\n",
    "#filename = f\"wave_data_{first_access_date}.csv\"\n",
    "#wave_df_with_index.to_csv(filename)\n",
    "#print(f\"\\nData saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603f5c9",
   "metadata": {},
   "source": [
    "### Read the data from a file\n",
    "\n",
    "**Justification:** Reading the previously saved wave data CSV back into a new dataframe (`saved_df`). This serves as a check to confirm that the file saving process (like in the previous commented-out cell) worked correctly and the data can be retrieved. For the main analysis continuity, operations will proceed using the primary dataframe (`wave_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "402252c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully read back from file.\n"
     ]
    }
   ],
   "source": [
    "# Read the data back from file\n",
    "saved_df = pd.read_csv(\"wave_data_2025-03-28.csv\")\n",
    "print(\"Data successfully read back from file.\")"
   ]
  }
 ],
 "metadata": {
  "creation_period": "",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nb_name": "template",
  "qut": {
   "creation_period": "2023_sem1",
   "nb_name": "template-assignment1",
   "unit_code": "IFN619"
  },
  "unit_code": "",
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
